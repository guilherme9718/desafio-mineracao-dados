{"timestamp": 1745267462.131187, "stored_source_code": "# Preparar para classifica\u00e7\u00e3o\nimport numpy as np\nimport pandas as pd\n# declare a list tasks whose products you want to use as inputs\nupstream = ['agrupar_dados']\nproduct = None\nnot_classification_columns = []\nfeature_selection_percentage = 0.8 # colunas que v\u00e3o restar ap\u00f3s a selecao de feature\ndimensionality_reduction_percentage = 0.1 # colunas que v\u00e3o restar ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade\ndimensionality_reduction_algoritm = 'svd' # pca ou svd\ndf_reg = pd.read_parquet(upstream['agrupar_dados']['data'])\ndf_reg\n## Sele\u00e7\u00e3o de features\nclassification_columns = df_reg.columns.difference(not_classification_columns)\ndf_X = df_reg[classification_columns]\ndf_y = df_reg['destaque']\ncategory_columns = [col for col in df_reg.columns if col.startswith( 'category_')]\nattributes_columns = [col for col in df_reg.columns if col.startswith( 'attribute_')]\nhours_columns = [col for col in df_reg.columns if col.startswith( 'business_open_')]\ngeo_columns = ['latitude', 'longitude']\nprint(\"category lenght:\", len(category_columns))\nprint(\"attributes lenght:\", len(attributes_columns))\nprint(\"hours lenght:\", len(hours_columns))\nprint(\"geo lenght:\", len(geo_columns))\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport math\n\ndef select_best(columns):\n    features_num = math.ceil(feature_selection_percentage * len(columns))\n    selector = SelectKBest(f_classif, k=features_num)\n    X_new = pd.DataFrame(selector.fit_transform(df_X[columns], df_y), \n                        columns=[list(df_X[columns].iloc[:, selector.get_support(indices=True)].columns)])\n    X_new.columns = [''.join(col) for col in X_new.columns.values]\n    return X_new\n\ndf_X_best_cat = select_best(category_columns)\ndf_X_best_attr = select_best(attributes_columns)\ndf_X_best_hours = select_best(hours_columns)\ndf_X_select = df_X.drop(category_columns + attributes_columns + hours_columns, axis=1)\ndf_X_select = pd.concat([df_X_select, df_X_best_attr, df_X_best_cat, df_X_best_hours], axis=1)\ndf_X_select\n## Redu\u00e7\u00e3o de dimensionalidade\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\ndef svd_reduction(columns, features_number):\n    svd = TruncatedSVD(n_components=features_number, n_iter=30, random_state=43)\n    return svd.fit_transform(df_pca_input[columns])\n    \ndef pca_reduction(columns, features_number):\n    svd = PCA(n_components=features_number)\n    return svd.fit_transform(df_pca_input[columns])\n\ndf_pca_input = df_X_select\nif dimensionality_reduction_algoritm == 'pca':\n    reduction_algorithm = pca_reduction\nelif dimensionality_reduction_algoritm == 'svd':\n    reduction_algorithm = svd_reduction\nelse:\n    raise f'algoritmo de redu\u00e7\u00e3o de dimensionalidade desconhecido: {dimensionality_reduction_algoritm}'\n\nfiltered_cat_cols = [col for col in df_pca_input.columns if col in category_columns]\nfiltered_attr_cols = [col for col in df_pca_input.columns if col in attributes_columns]\nfiltered_hours_cols = [col for col in df_pca_input.columns if col in hours_columns]\n\ndef reduce_dimensions(columns, columns_prefix): \n    features_number = math.ceil(len(columns) * dimensionality_reduction_percentage)\n    X_new = pd.DataFrame(reduction_algorithm(columns, features_number), \n                                       columns=[columns_prefix + str(x) for x in range(features_number)])\n    return X_new\n    \nif dimensionality_reduction_percentage < 1.0:\n    df_X_pca_cat = reduce_dimensions(filtered_cat_cols, 'cat_pca_')\n    df_X_pca_attr = reduce_dimensions(filtered_attr_cols, 'attr_pca_')\n    df_X_pca_hours = reduce_dimensions(filtered_hours_cols, 'hours_pca_')\n    df_X_pca_geo = reduce_dimensions(geo_columns, 'geolocalization_')\n    \n    df_X_pca = df_pca_input.drop(filtered_cat_cols + filtered_attr_cols + filtered_hours_cols + geo_columns, axis=1)\n    df_X_pca = pd.concat([df_X_pca, df_X_pca_attr, df_X_pca_cat, df_X_pca_hours, df_X_pca_geo], axis=1)\nelse:\n    df_X_pca = df_X\n\ndf_X_pca\ndf_X_pca.to_parquet(product['data_X'])\npd.DataFrame(df_y).to_parquet(product['data_y'])", "params": {"not_classification_columns": ["business_id", "attributes", "categories", "hours", "destaque", "graph_page_rank"], "feature_selection_percentage": 0.8, "dimensionality_reduction_percentage": 0.1, "dimensionality_reduction_algoritm": "svd"}}