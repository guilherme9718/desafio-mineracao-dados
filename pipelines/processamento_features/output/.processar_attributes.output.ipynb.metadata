{"timestamp": 1745197563.054164, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\nproduct = None\ndf_input_path = '../../dados/X_testToronto.csv'\nimport numpy as np\nimport pandas as pd\nimport ast\ndef literal_eval(x):\n    if pd.isna(x):\n        return dict()\n    raw_dict = ast.literal_eval(x)\n    for key in raw_dict:\n        raw_dict[key] = ast.literal_eval(raw_dict[key])\n    return raw_dict\n        \ndef proccess_df(df):\n    df['loc'] = df['loc'].apply(lambda x: ast.literal_eval(x))\n    df['attributes'] = df['attributes'].apply(literal_eval)\n    df['hours'] = df['hours'].apply(lambda x: ast.literal_eval(x) if not pd.isna(x) else dict())\ndf = pd.read_csv(df_input_path).set_index('business_id')\nproccess_df(df)\ndf_reg = df.copy()\ndf_reg = df_reg[['latitude', 'longitude', 'attributes', 'categories', 'hours']].reset_index()\ndf_reg\n### Transformar Attributes em dummy values\nfrom collections import defaultdict\n\nunique_attributes = defaultdict(set)\ntwo_layer_attributtes = defaultdict(set)\nattributes = df['attributes']\nfor item_dict in attributes:\n    for key in item_dict:\n        if type(item_dict[key]) is dict:\n            for key2 in item_dict[key]:\n                two_layer_attributtes[(key, key2)].add(item_dict[key][key2])\n        else:\n            unique_attributes[key].add(item_dict[key])\n\nfor attr1, attr2 in two_layer_attributtes:\n    if attr1 in unique_attributes:\n        unique_attributes.pop(attr1, None)\n\ndef dummy_attr(row):\n    atributes = {}\n    obj = row['attributes']\n    if row['attributes'] == None:\n        obj = dict()\n    for key in unique_attributes:\n        atributes[key] = None\n        if key in obj:\n            atributes[key] = obj[key]\n            \n    for attr1, attr2 in two_layer_attributtes:\n        key = f'{attr1}.{attr2}'\n        atributes[key] = None\n        if attr1 in obj and type(obj[attr1]) is dict and attr2 in obj[attr1]:\n            atributes[key] = obj[attr1][attr2]\n    return atributes\n\ndf_attr = pd.DataFrame.from_dict(list(df_reg.apply(dummy_attr, axis=1)))\ndf_almost_dummie = pd.concat([df_reg, df_attr], axis=1)\nnew_columns = {at: 'attribute_' + at for at in df_attr.columns}\ndf_almost_dummie = df_almost_dummie.rename(columns=new_columns)\n\ndf_dummies = pd.get_dummies(df_almost_dummie[new_columns.values()].astype(str))\ncolumns_to_drop = [x for x in list(df_dummies.columns) if ('_nan' in x or '_None' in x or '_none' in x)]\ndf_dummies = df_dummies.drop(columns_to_drop, axis=1)\n\ndf_reg = pd.concat([df_reg, df_dummies], axis=1)\ndf_reg\ndf_reg.to_parquet(product['data'])\nattributes_columns = [x for x in df_reg.columns if x.startswith('attribute_')]\nfrom IPython.display import display\nwith pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n    display(df_reg.iloc[13]['attributes'])\n    display(df_reg[attributes_columns].iloc[[13]])\n    display('----------------------------------')\n    display(df_reg.iloc[542]['attributes'])\n    display(df_reg[attributes_columns].iloc[[542]])\n    display('----------------------------------')\n    display(df_reg.iloc[5389]['attributes'])\n    display(df_reg[attributes_columns].iloc[[5389]])", "params": {"df_input_path": "../../dados/X_trainToronto.csv"}}