


import numpy as np
import pandas as pd
import ast


def literal_eval(x):
    if pd.isna(x):
        return dict()
    raw_dict = ast.literal_eval(x)
    for key in raw_dict:
        raw_dict[key] = ast.literal_eval(raw_dict[key])
    return raw_dict
        
def proccess_df(df):
    df['loc'] = df['loc'].apply(lambda x: ast.literal_eval(x))
    df['attributes'] = df['attributes'].apply(literal_eval)
    df['hours'] = df['hours'].apply(lambda x: ast.literal_eval(x) if not pd.isna(x) else dict())


df = pd.read_csv('dados/X_trainToronto.csv').set_index('business_id')
proccess_df(df)
df





df_reg = df.copy()
df_reg = df_reg[['latitude', 'longitude', 'attributes', 'categories', 'hours', 'destaque']].reset_index()
df_reg





from collections import defaultdict

unique_attributes = defaultdict(set)
two_layer_attributtes = defaultdict(set)
attributes = df['attributes']
for item_dict in attributes:
    for key in item_dict:
        if type(item_dict[key]) is dict:
            for key2 in item_dict[key]:
                two_layer_attributtes[(key, key2)].add(item_dict[key][key2])
        else:
            unique_attributes[key].add(item_dict[key])

for attr1, attr2 in two_layer_attributtes:
    if attr1 in unique_attributes:
        unique_attributes.pop(attr1, None)

def dummy_attr(row):
    atributes = {}
    obj = row['attributes']
    if row['attributes'] == None:
        obj = dict()
    for key in unique_attributes:
        atributes[key] = None
        if key in obj:
            atributes[key] = obj[key]
            
    for attr1, attr2 in two_layer_attributtes:
        key = f'{attr1}.{attr2}'
        atributes[key] = None
        if attr1 in obj and type(obj[attr1]) is dict and attr2 in obj[attr1]:
            atributes[key] = obj[attr1][attr2]
    return atributes

df_attr = pd.DataFrame.from_dict(list(df_reg.apply(dummy_attr, axis=1)))
df_almost_dummie = pd.concat([df_reg, df_attr], axis=1)

df_dummies = pd.get_dummies(df_almost_dummie[df_attr.columns].astype(str))
columns_to_drop = [x for x in list(df_dummies.columns) if ('_nan' in x or '_None' in x or '_none' in x)]
df_dummies = df_dummies.drop(columns_to_drop, axis=1)

df_reg = pd.concat([df_reg, df_dummies], axis=1)
df_reg





df_reg.categories = df_reg.categories.fillna('')
df_reg.categories = df_reg.categories.astype(str)
df_reg.categories = df_reg.categories.str.split(',')
df_reg.categories = df_reg.categories.apply(lambda x: [s.strip().lower() for s in x])


all_categories = defaultdict(int)
for c_list in df_reg.categories:
    if type(c_list) is float:
        c_list = []
    for c in c_list:
        all_categories[c.strip().lower()] = all_categories[c.strip().lower()] + 1

best_categories = pd.Series(all_categories)
best_categories = best_categories[best_categories > 50]
others_categories = set(all_categories.keys()).difference(set(best_categories.index))
best_categories


from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
df_cat_dummies = pd.DataFrame(mlb.fit_transform(df_reg.categories),columns=mlb.classes_, index=df_reg.index)
df_cat_dummies['others_categories'] = df_cat_dummies.apply(lambda x: x[list(others_categories)].sum(), axis=1)
df_cat_dummies = df_cat_dummies.drop(list(others_categories), axis=1)
df_reg = pd.concat([df_reg, df_cat_dummies], axis=1)





from datetime import datetime, timedelta

midnight_tomorrow = datetime.strptime("0:0", "%H:%M") + timedelta(days=1)
midnight_today = datetime.strptime("0:0", "%H:%M")
daysOfWeek = {'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'}
times_of_day = {
         'morning': (datetime.strptime('6:0', "%H:%M"), datetime.strptime('12:0', "%H:%M")), 
         'afternoon': (datetime.strptime('12:0', "%H:%M"), datetime.strptime('18:0', "%H:%M")), 
         'evening': (datetime.strptime('18:0', "%H:%M"), midnight_tomorrow),
         'night': (midnight_today, datetime.strptime('6:0', "%H:%M"))
        }

def fill_hours(obj):
    result = {}
    for dayOfweek in daysOfWeek:
        hour1 = datetime.strptime("0:0", "%H:%M")
        hour2 = datetime.strptime("0:0", "%H:%M")
        if dayOfweek in obj:
            hours_obj = obj[dayOfweek].split('-')
            hour1 = datetime.strptime(hours_obj[0], "%H:%M")
            hour2 = datetime.strptime(hours_obj[1], "%H:%M")
            if hour1 >= hour2:
                hour2 += timedelta(days=1)
        for key in times_of_day:
            d1 = times_of_day[key][0]
            d2 = times_of_day[key][1]
            
            value = calculate_hoursOfDay(hour1, hour2, d1, d2)
            result[f'business_open_{dayOfweek}_{key}'] = value
    return result
            
    
def calculate_hoursOfDay(hour1: datetime, hour2: datetime, d1: datetime, d2: datetime):
    if hour1 > d2 or hour2 < d1:
        if hour2.day == midnight_tomorrow.day and (hour1 != midnight_tomorrow and hour2 != midnight_tomorrow):
            day1 = calculate_hoursOfDay(midnight_tomorrow, hour2, d1 + timedelta(days=1), d2 + timedelta(days=1))
            day2 = calculate_hoursOfDay(hour1, midnight_tomorrow, d1, d2)
            return day1 + day2
        return 0.0
        
    inf = hour1
    sup = hour2
    if hour1 <= d1:
        inf = d1
    if hour2 >= d2:
        sup = d2
        
    return float((sup - inf).seconds) / 21600.0


hours_features = pd.DataFrame(list(df_reg['hours'].apply(fill_hours)))
df_reg = pd.concat([df_reg, hours_features], axis=1)
df_reg





df_review = pd.read_csv('dados_produzidos/reviewsTrainTorontoSentiment.csv')
df_review





import itertools
import networkx as nx

G = nx.Graph()
G.add_nodes_from(df.index)
    
def add_edges(rows):
    if len(rows) < 2:
        return
    business_ids = rows['business_id'].unique()
    for comb in itertools.combinations(business_ids, 2):
        if comb in G.edges:
            G.edges[comb[0], comb[1]]['weight'] += 1
        else:
            G.add_edge(comb[0], comb[1], weight=1)

df_review.groupby('user_id').apply(add_edges)





nx.write_gml(G, 'dados_produzidos/grafo_business_reviews.gml')





from sklearn.preprocessing import StandardScaler
data = df[['latitude', 'longitude']]
scaler = StandardScaler()
scaler.fit(data)
df_reg[['latitude', 'longitude']] = scaler.transform(data)
df_reg[['latitude', 'longitude']]





not_regression_columns = ['business_id', 'attributes', 'categories', 'hours', 'destaque', 'latitude', 'longitude']
df_X = df_reg[[x for x in df_reg.columns if x not in not_regression_columns]].apply(lambda x: x.astype(float))
df_y = df_reg['destaque']


from sklearn.decomposition import PCA
pca = PCA(n_components=10)
df_X = pd.DataFrame(pca.fit_transform(df_X))
df_X


from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=30, n_iter=7, random_state=42)
df_X = pd.DataFrame(svd.fit_transform(df_X))
df_X


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def measure(y_test, y_pred):
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy)
    
    # Calculate precision
    precision = precision_score(y_test, y_pred)
    print("Precision:", precision)
    
    # Calculate recall (sensitivity)
    recall = recall_score(y_test, y_pred)
    print("Recall (Sensitivity):", recall)
    
    # Calculate F1-score
    f1 = f1_score(y_test, y_pred)
    print("F1-Score:", f1)


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.25, random_state=892)
display(X_train)
display(y_train)


from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=1, max_depth=20)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
measure(y_test, y_pred)


from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(random_state=3).fit(X_train, y_train)
clf.score(X_train, y_train)


clf.predict_proba(X_test)


from sklearn.feature_selection import chi2

chi2_stats, p_values = chi2(X_train, y_train)

pd.Series(chi2_stats).sort_values()


X_train.iloc[:, 29]
